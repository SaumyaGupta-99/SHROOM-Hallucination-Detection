# SHROOM-Hallucination-Detection-In-LLMS
With the increasing use of Large Language Models (LLMs) like ChatGPT for various tasks such as question answering, machine translation, and text correction, ensuring the accuracy of their outputs is crucial. Our study addresses
the challenge of detecting hallucinated outputs from LLMs, which can be grammatically correct but factually incorrect or grammatically incorrect altogether for three types of tasks - definition modeling, paraphrasing and machine translation. I have used clustering tailored to different tasks using a combination of suitable metrics for hallucination detection. These approaches aim to improve the trustworthiness and accuracy of LLM-generated text, enhancing the reliability of AI-driven applications across domains.

## Tasks Considered

In our work, we have considered the following tasks:

1. **Definition Modeling**: Generating concise and accurate definitions or explanations of given terms or concepts.

2. **Paraphrasing**: Rewriting a given text or sentence while preserving its original meaning and intent.

3. **Machine Translation**: Automatically translating text from one language to another using computational algorithms or models.

## Dataset
The dataset utilized in our study is taken from the SemEval 2024 task named Shroom. It consists of three main subsets: training, trial, and testing data. Our dataset incorporates fields such as source which is the data provided
to the LLMs to produce the result including and command (specifying the instruction given to the LLMs), this source data is instrumental in factually verifying our hypothesis data. Additionally, the dataset includes target data, repre-
senting the expected output from the LLMs, and hypothesis data, denoting the actual output generated by the LLMs. Each instance in the dataset is associated with a specific task, namely definition modeling, paraphrasing, or machine
translation.

## Methodology
We have used a combination of different metrics and performed clustering using these metrics to detect if the output is hallucinated or not. Using plain cosine similarity and setting a threshold on that is not a sufficient and accurate
approach for detecting hallucinations as it can’t capture all the language semantics, syntax, and the context. Our tasks - Paraphrasing, Definition Modeling, and Machine Translation require more sophisticated metrics tailored to their specific requirements. We have considered a combination of metrics here, each tailored specifically to these tasks. The different metrics we have considered are - **Cosine Similarity, Summary Scores, ROUGE, ROUGE, BLEU, Jaccard Similairy , Dice Similarity, BERT Score, Sentence Entropy, Sentiment Analysis, COMET SCORE , TER SCORE**. Depending on different tasks different combinations of these scores have been considered. Clustering algorithms using a combination of
these metrics for different tasks capture different aspects of our data. This offers flexibility, and as clustering is data-driven, it can adapt to the distribution of data. This does not require predefined thresholds, which are
challenging to determine, especially in our case because of the absence of ground truth labels. We have used different types of clustering methods: **K-Means, Spectral Clustering, Birch, Gaussian, Hierarchical, and Mean**. **Additionally, we have experimented with clustering based on individual metrics using K-Means, selecting optimal clusters via inertia, and employing a voting mechanism to determine the predominant label among the clusters**. Each
clustering method offers distinct advantages, ranging from partitioning the data into spherical clusters (K-Means) to spectral decomposition for non-linear separation (SpectralClustering).

## Results
**F1 Score Plot**
![F1 score Plot](https://github.com/SaumyaGupta-99/SHROOM-Hallucination-Detection/blob/main/f1_score_plot.png)
** Accuracy Score Plot*
![Accuracy score Plot](https://github.com/SaumyaGupta-99/SHROOM-Hallucination-Detection/blob/main/accuracy_plot.png)
 **Examples**
<br>  <br>
**TASK: DEFINITION MODELLING**
<br>
Source: It was what had attracted me to her in the first place. "You can say something," I said. "I'm easy to talk to." She frowned . The last time we had a private talk, in her kitchen , she admitted to playing me for a fool,making me fall in love with her , and breadcrumbing my way to discovering a deadly necromantic artifact.
<br>
Expected Target: To use clues or enticements to lead someone in the desired direction.
 <br>
Hypothesis: To trick or deceive.
 <br>
True Label: Hallucination
 <br>
Model Output: Hallucination
 <br>
 <br>
**TASK: PARAPHRASE GENERATION**
 <br>
Source: The Rules do not allow two committees to be responsible for the same matter.
 <br>
Hypothesis: Two committees can’t be held responsible for the same thing.
 <br>
True Label: Not Hallucination
 <br>
Model Output: Not Hallucination
 <br>
<br>
**TASK: MACHINE TRANSLATION**
 <br>
Expected Target: They also define routes for popular hiking and cycling trails.
 <br>
Hypothesis: Inland waterways also define common ways of walking and riding bicycles.
 <br>
True Label: Hallucination
 <br>
Model Output: Hallucination
 <br>

## More Implemenations for Hallucination Detection
We have also tried other implemenations like - training a Siamese network using BERT model and
cosine similarity and employing prompting with the LLAMA model. All these implementations can be found here - https://github.com/anudeepragata/SHROOM
